{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\nImportar librerias<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import seaborn as sns\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.neural_network import MLPClassifier\n", "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n", "import shap  # Instalar la librer\u00c3\u00ada SHAP si no est\u00c3\u00a1 instalada\n", "import matplotlib.pyplot as plt\n", "import matplotlib\n", "matplotlib.use('TkAgg')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nS6.T0. Importar la base de datos a PyCharm<br>\n", "\u00ef\u0192\u02dc Guarda la base de datos BBDD_ST5.csv en un dataframe llamado \"df\"<br>\n", "\u00ef\u0192\u02dc El dtype de las columnas indicadas a continuaci\u00c3\u00b3n las convertimos de float64/int64 a float32/int32 para reducir memoria<br>\n", "\u00ef\u0192\u02dc Separa los inputs/output en set de entrenamiento (train: 80 %) y validaci\u00c3\u00b3n (test: 20 %) con la funci\u00c3\u00b3n train_test_split.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(r'E:\\Laptop\\BBDD_ST5.csv')  # Importar la base de datos BBDD_ST5.csv"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["float64_cols = df.select_dtypes(include='float64').columns.tolist()  # Seleccionar las columnas de tipo float64\n", "df[float64_cols] = df[float64_cols].astype('float32')  # Convertir las columnas de tipo float64 a float32\n", "int64_cols = df.select_dtypes(include='int64').columns.tolist()  # Seleccionar las columnas de tipo int64\n", "df[int64_cols] = df[int64_cols].astype('int32')  # Convertir las columnas de tipo int64 a int32"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_train, x_test, y_train, y_test = train_test_split(\n", "    df.drop(\"Qualificaci\u00c3\u00b3 de consum d'energia primaria no renovable\", axis=1),  # Inputs\n", "    df[\"Qualificaci\u00c3\u00b3 de consum d'energia primaria no renovable\"],  # Output\n", "    test_size=0.2,  # Tama\u00c3\u00b1o del set de validaci\u00c3\u00b3n (20 %)\n", "    random_state=1)  # Semilla para controlar la aleatoriedad. No tocar"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nS6.T1. Construcci\u00c3\u00b3n de un clasificador basado en \"ANN con softmax activation\"<br>\n", "\u00ef\u0192\u02dc Consigue entrenar un modelo ANN con tus datos (df).<br>\n", "\u00ef\u0192\u02dc Compara los resultados del \"clasification report\" de tu modelo en el test de validaci\u00c3\u00b3n con los valores de referencia:<br>\n", "               precision    recall  f1-score   support<br>\n", "           0       0.56      0.57      0.56     32389<br>\n", "           1       0.00      0.00      0.00     18259<br>\n", "           2       0.65      0.85      0.74     75197<br>\n", "           3       0.30      0.29      0.30     13190<br>\n", "           4       0.34      0.08      0.13      5191<br>\n", "           5       0.59      0.27      0.37      1143<br>\n", "           6       0.53      0.20      0.29       355<br>\n", "    accuracy                           0.60    145724<br>\n", "   macro avg       0.42      0.32      0.34    145724<br>\n", "C\u00c3\u00b3mo variable resumen del comportamiento de vuestro modelo, os pod\u00c3\u00a9is fijar en el accuracy, que en este caso es del 57 %.<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nPuedes modificar estas variables para evaluar distintas configuraciones de la red neuronal\n<br>\n", "neurons_hidden_layers = (10, 20)  # N\u00c3\u00bamero de neuronas en cada capa. En este caso: 10 neuronas en la primera capa y 10 neuronas en la segunda capa.<br>\n", "max_iter = 20  # N\u00c3\u00bamero de iteraciones para el entrenamiento del modelo (epochs).<br>\n", "class_weight = {0: 1.63, 1: 2.91, 2: 0.7, 3: 4.05, 4: 10.3, 5: 44.3, 6: 142.2}  # Para compensar el desbalance de clases.<br>\n", "in de la variables a modificar\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nCrear el modelo de red neuronal\n<br>\n", "mlp = MLPClassifier(<br>\n", "    hidden_layer_sizes=neurons_hidden_layers,<br>\n", "    activation='relu',<br>\n", "    solver='adam',<br>\n", "    max_iter=max_iter,<br>\n", "    learning_rate_init=0.00005,<br>\n", "    random_state=1234,<br>\n", "    verbose=1  # Muestra detalles durante el entrenamiento<br>\n", ")<br>\n", "ntrenar el modelo con el set de entrenamiento\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["history = mlp.fit(x_train, y_train)  # Entrenar el modelo con el set de entrenamiento"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\nVisualizar m\u00c3\u00a9tricas del entrenamiento.\n<br>\n", "train_loss = history.loss_curve_<br>\n", "# Calcular accuracy por epoch en el set de entrenamiento<br>\n", "train_accuracy = []<br>\n", "for i in range(len(train_loss)):<br>\n", "    y_train_pred = mlp.predict(x_train)<br>\n", "    acc = accuracy_score(y_train, y_train_pred)<br>\n", "    train_accuracy.append(acc)<br>\n", "# Graficar el rendimiento por epoch<br>\n", "epochs = range(1, len(train_loss) + 1)<br>\n", "plt.figure(figsize=(12, 6))<br>\n", "# Gr\u00c3\u00a1fico de la p\u00c3\u00a9rdida (loss)<br>\n", "plt.subplot(1, 2, 1)<br>\n", "plt.plot(epochs, train_loss, label='Training Loss', color='blue')<br>\n", "plt.xlabel('Epochs')<br>\n", "plt.ylabel('Loss')<br>\n", "plt.title('Training Loss per Epoch')<br>\n", "plt.grid(alpha=0.3)<br>\n", "plt.legend()<br>\n", "# Gr\u00c3\u00a1fico del accuracy<br>\n", "plt.subplot(1, 2, 2)<br>\n", "plt.plot(epochs, train_accuracy, label='Training Accuracy', color='green')<br>\n", "plt.xlabel('Epochs')<br>\n", "plt.ylabel('Accuracy')<br>\n", "plt.title('Training Accuracy per Epoch')<br>\n", "plt.grid(alpha=0.3)<br>\n", "plt.legend()<br>\n", "plt.tight_layout()<br>\n", "plt.show()<br>\n", "valuar el modelo con el set de validaci\u00c3\u00b3n. Imprimir el accuracy para el set de entrenamiento y validaci\u00c3\u00b3n. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "y_pred = mlp.predict(x_test)  # Predecir el set de validaci\u00c3\u00b3n<br>\n", "print(classification_report(y_test, y_pred))  # Imprimir el classification report<br>\n", "print(classification_report(y_train, mlp.predict(x_train)))  # Imprimir el classification report<br>\n", "onfusion matrix on test set"]}, {"cell_type": "markdown", "metadata": {}, "source": ["    \n<br>\n", "cm = confusion_matrix(y_test, y_pred)<br>\n", "plt.figure(figsize=(10, 7))<br>\n", "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"G\", \"F\", \"E\", \"D\", \"C\", \"B\", \"A\"], yticklabels=[\"G\", \"F\", \"E\", \"D\", \"C\", \"B\", \"A\"])<br>\n", "plt.xlabel('Predicted Labels')<br>\n", "plt.ylabel('True Labels')<br>\n", "plt.title('Confusion Matrix')<br>\n", "plt.show()<br>\n", "HAP values"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}